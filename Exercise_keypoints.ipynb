{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOGAir0wIOgdRoTqIfNlOdn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chathurya99/Pose_Fit_FYP/blob/master/Exercise_keypoints.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        },
        "id": "h-KXZHlYF-pB",
        "outputId": "b8ab5a15-ce7b-4e9c-df79-7a8a6e656067"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading person detector model from https://tfhub.dev/tensorflow/ssd_mobilenet_v2/2...\n",
            "Person detector model loaded successfully.\n",
            "Loading MoveNet Thunder model from https://tfhub.dev/google/movenet/singlepose/thunder/4...\n",
            "MoveNet Thunder model loaded successfully.\n",
            "Please upload your exercise video file:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-cb37dcc9-bb29-4257-acdf-20d3fd882f2a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-cb37dcc9-bb29-4257-acdf-20d3fd882f2a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving lunges.mp4 to lunges.mp4\n",
            "\n",
            "Video 'lunges.mp4' uploaded successfully.\n",
            "\n",
            "Starting keypoint extraction with keypoint-guided tracking...\n",
            "IMPORTANT: Video processing at: 768x432\n",
            "Target CSV canvas: 640x480\n",
            "Using tracking_crop_padding_factor: 1.7\n",
            "Movenet confidence threshold for tracking: 0.2\n",
            "Frame 1 Debug: Nose (abs_x, abs_y) on 768x432 frame BEFORE final scaling to canvas: (413.69, 100.52) -> CSV Nose_X: 344.74\n",
            "Frame 2 Debug: Nose (abs_x, abs_y) on 768x432 frame BEFORE final scaling to canvas: (415.02, 42.02) -> CSV Nose_X: 345.85\n",
            "Frame 3 Debug: Nose (abs_x, abs_y) on 768x432 frame BEFORE final scaling to canvas: (413.07, 88.19) -> CSV Nose_X: 344.22\n",
            "Frame 4 Debug: Nose (abs_x, abs_y) on 768x432 frame BEFORE final scaling to canvas: (412.43, 45.51) -> CSV Nose_X: 343.70\n",
            "Frame 5 Debug: Nose (abs_x, abs_y) on 768x432 frame BEFORE final scaling to canvas: (410.97, 85.64) -> CSV Nose_X: 342.48\n",
            "Processed 50 frames...\n",
            "Processed 100 frames...\n",
            "Processed 150 frames...\n",
            "Finished processing. Total frames processed: 170\n",
            "\n",
            "Keypoints saved to 'exercise_keypoints_tracked_pad1.7_conf0.2_lunges.csv'\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_3e3fe0ec-7872-41cb-ae57-88c9435f08cb\", \"exercise_keypoints_tracked_pad1.7_conf0.2_lunges.csv\", 57187)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Download initiated.\n",
            "IMPORTANT: Check the number of rows in the output CSV. It should be closer to 2175 now.\n",
            "If values are still off, the main parameter to tune is 'ADJUSTED_TRACKING_CROP_PADDING_FACTOR' (currently 1.7).\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# --- Model Loading ---\n",
        "# Person Detector (SSD MobileNet V2 from TF Hub)\n",
        "detector_hub_url = \"https://tfhub.dev/tensorflow/ssd_mobilenet_v2/2\"\n",
        "print(f\"Loading person detector model from {detector_hub_url}...\")\n",
        "try:\n",
        "    detector_model = hub.load(detector_hub_url)\n",
        "    print(\"Person detector model loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading person detector model: {e}\")\n",
        "    raise\n",
        "\n",
        "# MoveNet Thunder model\n",
        "movenet_hub_url = \"https://tfhub.dev/google/movenet/singlepose/thunder/4\"\n",
        "print(f\"Loading MoveNet Thunder model from {movenet_hub_url}...\")\n",
        "try:\n",
        "    movenet_pose_model = hub.load(movenet_hub_url)\n",
        "    movenet_input_size = 256 # Expected input size for Movenet\n",
        "    print(\"MoveNet Thunder model loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading MoveNet model: {e}\")\n",
        "    raise\n",
        "\n",
        "# --- Helper Functions ---\n",
        "\n",
        "def run_detector(detector, image_tensor_rgb):\n",
        "    \"\"\"Runs person detection on an input RGB image tensor.\"\"\"\n",
        "    if image_tensor_rgb.dtype != tf.uint8:\n",
        "        image_tensor_rgb = tf.cast(image_tensor_rgb, tf.uint8)\n",
        "    if len(image_tensor_rgb.shape) == 3: # If single image\n",
        "        image_tensor_rgb = tf.expand_dims(image_tensor_rgb, axis=0)\n",
        "    detector_output = detector(image_tensor_rgb)\n",
        "    result = {key: value.numpy() for key, value in detector_output.items()}\n",
        "    return result\n",
        "\n",
        "def run_movenet_on_crop(pose_model, cropped_image_rgb_tf, movenet_target_size):\n",
        "    \"\"\"Runs MoveNet pose estimation on a cropped RGB image tensor.\"\"\"\n",
        "    movenet_input_img = tf.image.resize_with_pad(cropped_image_rgb_tf, movenet_target_size, movenet_target_size)\n",
        "    movenet_input_img = tf.cast(movenet_input_img, dtype=tf.int32)\n",
        "    if len(movenet_input_img.shape) == 3: # If single image\n",
        "         movenet_input_img = tf.expand_dims(movenet_input_img, axis=0)\n",
        "    model_output = pose_model.signatures['serving_default'](movenet_input_img)\n",
        "    return model_output['output_0'] # Keypoints with scores\n",
        "\n",
        "def get_crop_from_bbox(image_np, bbox_normalized, padding_factor):\n",
        "    \"\"\"\n",
        "    Creates a square crop from the image based on the normalized bounding box.\n",
        "    Args:\n",
        "        image_np: Original image (H, W, C).\n",
        "        bbox_normalized: [ymin, xmin, ymax, xmax] normalized coordinates for the base box.\n",
        "        padding_factor: Factor to expand the crop around the max dimension of the bbox.\n",
        "    Returns:\n",
        "        cropped_image_np: The cropped image region.\n",
        "        crop_details: Dict with crop's top-left (x1, y1) in original frame and crop's W, H.\n",
        "    \"\"\"\n",
        "    img_h, img_w = image_np.shape[:2]\n",
        "    ymin, xmin, ymax, xmax = bbox_normalized\n",
        "\n",
        "    # Denormalize to pixel coordinates\n",
        "    xmin_px = int(xmin * img_w)\n",
        "    xmax_px = int(xmax * img_w)\n",
        "    ymin_px = int(ymin * img_h)\n",
        "    ymax_px = int(ymax * img_h)\n",
        "\n",
        "    box_w = xmax_px - xmin_px\n",
        "    box_h = ymax_px - ymin_px\n",
        "\n",
        "    if box_w <= 0 or box_h <= 0: return None, None\n",
        "\n",
        "    center_x = xmin_px + box_w / 2.0\n",
        "    center_y = ymin_px + box_h / 2.0\n",
        "\n",
        "    # Determine the side length of the square crop\n",
        "    crop_edge_length = max(box_w, box_h) * padding_factor\n",
        "    crop_edge_half = crop_edge_length / 2.0\n",
        "\n",
        "    # Calculate intended crop boundaries\n",
        "    intended_y1 = int(center_y - crop_edge_half)\n",
        "    intended_y2 = int(center_y + crop_edge_half)\n",
        "    intended_x1 = int(center_x - crop_edge_half)\n",
        "    intended_x2 = int(center_x + crop_edge_half)\n",
        "\n",
        "    # Clip coordinates to be within image bounds for the actual crop\n",
        "    actual_y1 = max(0, intended_y1)\n",
        "    actual_y2 = min(img_h, intended_y2)\n",
        "    actual_x1 = max(0, intended_x1)\n",
        "    actual_x2 = min(img_w, intended_x2)\n",
        "\n",
        "    if actual_y2 <= actual_y1 or actual_x2 <= actual_x1: return None, None\n",
        "\n",
        "    cropped_image_np = image_np[actual_y1:actual_y2, actual_x1:actual_x2]\n",
        "\n",
        "    if cropped_image_np.shape[0] == 0 or cropped_image_np.shape[1] == 0: return None, None\n",
        "\n",
        "    crop_details_dict = {\n",
        "        \"x1_original_frame\": actual_x1,\n",
        "        \"y1_original_frame\": actual_y1,\n",
        "        \"width_of_crop\": cropped_image_np.shape[1],\n",
        "        \"height_of_crop\": cropped_image_np.shape[0]\n",
        "    }\n",
        "    return cropped_image_np, crop_details_dict\n",
        "\n",
        "def get_bbox_from_keypoints(keypoints_abs_coords, img_width, img_height):\n",
        "    \"\"\"\n",
        "    Calculates a bounding box from absolute keypoint coordinates.\n",
        "    Returns normalized [ymin, xmin, ymax, xmax].\n",
        "    Filters out keypoints with NaN coordinates.\n",
        "    \"\"\"\n",
        "    valid_x = [kp[0] for kp in keypoints_abs_coords if not np.isnan(kp[0])]\n",
        "    valid_y = [kp[1] for kp in keypoints_abs_coords if not np.isnan(kp[1])]\n",
        "\n",
        "    if not valid_x or not valid_y:\n",
        "        return None\n",
        "\n",
        "    xmin_px = min(valid_x)\n",
        "    xmax_px = max(valid_x)\n",
        "    ymin_px = min(valid_y)\n",
        "    ymax_px = max(valid_y)\n",
        "\n",
        "    # Normalize\n",
        "    bbox_norm = [\n",
        "        ymin_px / img_height,\n",
        "        xmin_px / img_width,\n",
        "        ymax_px / img_height,\n",
        "        xmax_px / img_width\n",
        "    ]\n",
        "    return bbox_norm\n",
        "\n",
        "\n",
        "def extract_keypoints_video_tracked(\n",
        "    video_path, detector, pose_estimator, movenet_dim,\n",
        "    target_csv_canvas_width, target_csv_canvas_height,\n",
        "    initial_person_score_thresh=0.4, # For the general detector\n",
        "    movenet_confidence_thresh=0.2,   # Avg score of MoveNet keypoints to consider track valid\n",
        "    tracking_crop_padding_factor=1.7 # Padding for the crop (both initial and tracked)\n",
        "    ):\n",
        "\n",
        "    all_frames_data = []\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frame_idx = 0\n",
        "\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: Could not open video file {video_path}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    original_vid_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    original_vid_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    print(f\"IMPORTANT: Video processing at: {original_vid_width}x{original_vid_height}\")\n",
        "    print(f\"Target CSV canvas: {target_csv_canvas_width}x{target_csv_canvas_height}\")\n",
        "    print(f\"Using tracking_crop_padding_factor: {tracking_crop_padding_factor}\")\n",
        "    print(f\"Movenet confidence threshold for tracking: {movenet_confidence_thresh}\")\n",
        "\n",
        "    last_tracked_person_bbox_normalized = None # Store the bbox from previous successful frame\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame_bgr_np = cap.read()\n",
        "        if not ret: break\n",
        "        frame_idx += 1\n",
        "\n",
        "        current_keypoints_for_csv_row = [frame_idx] # Start with frame number\n",
        "        nan_keypoints_row = [frame_idx] + [np.nan] * 34 # Predefined row for failures\n",
        "\n",
        "        frame_rgb_np = cv2.cvtColor(frame_bgr_np, cv2.COLOR_BGR2RGB)\n",
        "        frame_rgb_tf = tf.convert_to_tensor(frame_rgb_np)\n",
        "\n",
        "        base_bbox_for_crop = None # This will be [ymin, xmin, ymax, xmax] normalized\n",
        "\n",
        "        # --- Determine Bounding Box for Cropping ---\n",
        "        if last_tracked_person_bbox_normalized:\n",
        "            base_bbox_for_crop = last_tracked_person_bbox_normalized\n",
        "            # print(f\"Frame {frame_idx}: Using last tracked bbox for crop.\")\n",
        "        else:\n",
        "            # Try to detect with general person detector (e.g., first frame or after track loss)\n",
        "            # print(f\"Frame {frame_idx}: No tracked bbox, running general person detector.\")\n",
        "            detected_objects = run_detector(detector, frame_rgb_tf)\n",
        "            person_boxes = []\n",
        "            person_scores_list = []\n",
        "            for i in range(int(detected_objects['num_detections'][0])):\n",
        "                class_id = int(detected_objects['detection_classes'][0][i])\n",
        "                score = float(detected_objects['detection_scores'][0][i])\n",
        "                if class_id == 1 and score >= initial_person_score_thresh:\n",
        "                    person_boxes.append(detected_objects['detection_boxes'][0][i])\n",
        "                    person_scores_list.append(score)\n",
        "\n",
        "            if person_boxes:\n",
        "                base_bbox_for_crop = person_boxes[np.argmax(person_scores_list)] # Highest score person\n",
        "                # print(f\"Frame {frame_idx}: Person detected by general detector.\")\n",
        "            else:\n",
        "                # No person detected by general detector\n",
        "                if frame_idx % 50 == 0 or frame_idx == 1:\n",
        "                    print(f\"Frame {frame_idx}: General detector found no person. Outputting NaNs.\")\n",
        "                all_frames_data.append(nan_keypoints_row[:])\n",
        "                last_tracked_person_bbox_normalized = None # Ensure re-detection next time\n",
        "                continue\n",
        "\n",
        "        # --- Create Crop based on base_bbox_for_crop ---\n",
        "        person_crop_np, crop_info = get_crop_from_bbox(\n",
        "            frame_rgb_np, base_bbox_for_crop, padding_factor=tracking_crop_padding_factor\n",
        "        )\n",
        "\n",
        "        if person_crop_np is None or person_crop_np.size == 0:\n",
        "            if frame_idx % 50 == 0 or frame_idx == 1:\n",
        "                print(f\"Frame {frame_idx}: Failed to create valid crop. Outputting NaNs.\")\n",
        "            all_frames_data.append(nan_keypoints_row[:])\n",
        "            last_tracked_person_bbox_normalized = None # Trigger re-detection\n",
        "            continue\n",
        "\n",
        "        # --- Run MoveNet on the Crop ---\n",
        "        person_crop_tf = tf.convert_to_tensor(person_crop_np)\n",
        "        keypoints_movenet_output = run_movenet_on_crop(pose_estimator, person_crop_tf, movenet_dim)\n",
        "        keypoints_norm_crop = np.squeeze(keypoints_movenet_output.numpy()) # (17,3) y,x,score\n",
        "\n",
        "        # --- Evaluate MoveNet Output and Update Tracking ---\n",
        "        avg_movenet_score = np.mean(keypoints_norm_crop[:, 2])\n",
        "\n",
        "        if avg_movenet_score < movenet_confidence_thresh:\n",
        "            if frame_idx % 50 == 0 or frame_idx == 1:\n",
        "                print(f\"Frame {frame_idx}: MoveNet confidence too low ({avg_movenet_score:.2f}). Outputting NaNs.\")\n",
        "            all_frames_data.append(nan_keypoints_row[:])\n",
        "            last_tracked_person_bbox_normalized = None # Lost track, trigger re-detection\n",
        "            continue\n",
        "\n",
        "        # Track successful, process keypoints\n",
        "        keypoints_abs_for_next_bbox = [] # Store abs (x,y) in original frame for next bbox calc\n",
        "\n",
        "        # For Debugging: Print abs coords for the first keypoint (nose) for the first few frames\n",
        "        first_keypoint_abs_x_debug, first_keypoint_abs_y_debug = np.nan, np.nan\n",
        "\n",
        "        for i in range(keypoints_norm_crop.shape[0]): # For each of 17 keypoints\n",
        "            y_norm_on_crop_input = keypoints_norm_crop[i, 0]\n",
        "            x_norm_on_crop_input = keypoints_norm_crop[i, 1]\n",
        "\n",
        "            x_px_in_crop = x_norm_on_crop_input * crop_info['width_of_crop']\n",
        "            y_px_in_crop = y_norm_on_crop_input * crop_info['height_of_crop']\n",
        "\n",
        "            x_abs_orig = x_px_in_crop + crop_info['x1_original_frame']\n",
        "            y_abs_orig = y_px_in_crop + crop_info['y1_original_frame']\n",
        "            keypoints_abs_for_next_bbox.append((x_abs_orig, y_abs_orig))\n",
        "\n",
        "            if i == 0 and frame_idx <= 5: # For nose keypoint (index 0) and first 5 frames\n",
        "                first_keypoint_abs_x_debug = x_abs_orig\n",
        "                first_keypoint_abs_y_debug = y_abs_orig\n",
        "\n",
        "            # Scale to target CSV canvas\n",
        "            csv_x = (x_abs_orig / original_vid_width) * target_csv_canvas_width\n",
        "            csv_y = (y_abs_orig / original_vid_height) * target_csv_canvas_height\n",
        "            current_keypoints_for_csv_row.extend([csv_x, csv_y])\n",
        "\n",
        "        if frame_idx <= 5 : # Print debug for first 5 frames\n",
        "             print(f\"Frame {frame_idx} Debug: Nose (abs_x, abs_y) on {original_vid_width}x{original_vid_height} frame BEFORE final scaling to canvas: ({first_keypoint_abs_x_debug:.2f}, {first_keypoint_abs_y_debug:.2f}) -> CSV Nose_X: {current_keypoints_for_csv_row[1]:.2f}\")\n",
        "\n",
        "        all_frames_data.append(current_keypoints_for_csv_row)\n",
        "\n",
        "        # Update last_tracked_person_bbox_normalized for the next frame\n",
        "        last_tracked_person_bbox_normalized = get_bbox_from_keypoints(keypoints_abs_for_next_bbox, original_vid_width, original_vid_height)\n",
        "        if last_tracked_person_bbox_normalized is None:\n",
        "             # This can happen if all keypoints were NaN (already handled by confidence check)\n",
        "             # or if min/max calculation failed. Trigger re-detection.\n",
        "            if frame_idx % 50 == 0 or frame_idx == 1:\n",
        "                print(f\"Frame {frame_idx}: Could not derive bbox from keypoints. Will re-detect.\")\n",
        "\n",
        "\n",
        "        if frame_idx % 50 == 0:\n",
        "            print(f\"Processed {frame_idx} frames...\")\n",
        "\n",
        "    cap.release()\n",
        "    print(f\"Finished processing. Total frames processed: {frame_idx}\")\n",
        "\n",
        "    keypoint_names_ordered = [\n",
        "        \"nose\", \"left_eye\", \"right_eye\", \"left_ear\", \"right_ear\",\n",
        "        \"left_shoulder\", \"right_shoulder\", \"left_elbow\", \"right_elbow\",\n",
        "        \"left_wrist\", \"right_wrist\", \"left_hip\", \"right_hip\",\n",
        "        \"left_knee\", \"right_knee\", \"left_ankle\", \"right_ankle\"\n",
        "    ]\n",
        "    csv_columns = [\"frame\"]\n",
        "    for name in keypoint_names_ordered:\n",
        "        csv_columns.append(f\"{name}_x\"); csv_columns.append(f\"{name}_y\")\n",
        "    output_df = pd.DataFrame(all_frames_data, columns=csv_columns)\n",
        "    return output_df\n",
        "\n",
        "# --- Main script execution ---\n",
        "TARGET_CSV_CANVAS_WIDTH = 640\n",
        "TARGET_CSV_CANVAS_HEIGHT = 480 # Your target canvas for the final CSV\n",
        "\n",
        "# <<<< --- ADJUST THIS PADDING FACTOR --- >>>>\n",
        "# This factor controls the \"zoom\" level of the crop around the person (tracked or initially detected).\n",
        "# Larger values mean a looser crop (person is smaller in the view fed to MoveNet).\n",
        "# Smaller values mean a tighter crop.\n",
        "# Based on previous results, we might need a value around 1.7 to 2.0 or higher if current values are too small.\n",
        "# If current values are too large, decrease this.\n",
        "ADJUSTED_TRACKING_CROP_PADDING_FACTOR = 1.7 # Initial guess, adjust this based on output\n",
        "\n",
        "# Confidence for initial person detection by the general detector\n",
        "INITIAL_PERSON_DETECTION_THRESHOLD = 0.3 # Lowered slightly to catch more initial poses\n",
        "\n",
        "# Average confidence of MoveNet keypoints to consider the track \"valid\" for the next frame\n",
        "MOVENET_TRACKING_CONFIDENCE_THRESHOLD = 0.2 # Avg score of all 17 keypoints\n",
        "\n",
        "print(\"Please upload your exercise video file:\")\n",
        "uploaded_video_colab = files.upload()\n",
        "\n",
        "if not uploaded_video_colab:\n",
        "    print(\"No video file was uploaded. Exiting.\")\n",
        "else:\n",
        "    video_input_filename = list(uploaded_video_colab.keys())[0]\n",
        "    print(f\"\\nVideo '{video_input_filename}' uploaded successfully.\")\n",
        "    print(\"\\nStarting keypoint extraction with keypoint-guided tracking...\")\n",
        "\n",
        "    keypoints_result_df = extract_keypoints_video_tracked(\n",
        "        video_input_filename,\n",
        "        detector_model,\n",
        "        movenet_pose_model,\n",
        "        movenet_input_size,\n",
        "        TARGET_CSV_CANVAS_WIDTH,\n",
        "        TARGET_CSV_CANVAS_HEIGHT,\n",
        "        initial_person_score_thresh=INITIAL_PERSON_DETECTION_THRESHOLD,\n",
        "        movenet_confidence_thresh=MOVENET_TRACKING_CONFIDENCE_THRESHOLD,\n",
        "        tracking_crop_padding_factor=ADJUSTED_TRACKING_CROP_PADDING_FACTOR\n",
        "    )\n",
        "\n",
        "    if not keypoints_result_df.empty:\n",
        "        output_csv_filename = (f\"exercise_keypoints_tracked_pad{ADJUSTED_TRACKING_CROP_PADDING_FACTOR}_\"\n",
        "                               f\"conf{MOVENET_TRACKING_CONFIDENCE_THRESHOLD}_{video_input_filename.split('.')[0]}.csv\")\n",
        "        keypoints_result_df.to_csv(output_csv_filename, index=False)\n",
        "        print(f\"\\nKeypoints saved to '{output_csv_filename}'\")\n",
        "        files.download(output_csv_filename)\n",
        "        print(\"\\nDownload initiated.\")\n",
        "        print(f\"IMPORTANT: Check the number of rows in the output CSV. It should be closer to {2175} now.\")\n",
        "        print(f\"If values are still off, the main parameter to tune is 'ADJUSTED_TRACKING_CROP_PADDING_FACTOR' (currently {ADJUSTED_TRACKING_CROP_PADDING_FACTOR}).\")\n",
        "    else:\n",
        "        print(\"No keypoints were extracted. The DataFrame is empty.\")"
      ]
    }
  ]
}